# GTMind â€” Autonomous Research Agent ğŸ”

> **From a single query to a structured, sourceâ€‘linked research report.**  
> Searches the web â†’ cleans articles â†’ extracts insights with GPTâ€‘4o â†’ deâ€‘duplicates & ranks â†’ serves JSON, SQLite, CLI, and a Streamlit UI.

---

## âœ¨ What's inside?

| Layer | Module | Tech / Libs | Purpose |
|-------|--------|-------------|---------|
| **Search**      | `core/search.py`      | Serper.dev Â· `httpx`          | Googleâ€‘style search â†’ URLs |
| **Parse**       | `core/parse.py`       | Async `httpx` Â· `trafilatura` | Download & boilerplateâ€‘strip HTML |
| **Extract**     | `core/extract.py`     | OpenAI GPTâ€‘4o Â· functionâ€‘calling | Pull trends, companies, gaps |
| **Aggregate**   | `core/aggregate.py`   | RapidFuzz Â· frequency rank    | Deâ€‘dupe & merge into a report |
| **Persist**     | `persistence.py`      | SQLite Â· `sqlmodel`           | `--save-sqlite` flag, history sidebar |
| **Serve**       | `api/run.py`          | FastAPI Â· Typer CLI           | `/report` JSON Â· `gtmind` CLI cmd |
| **UI**          | `ui/app.py`           | Streamlit                     | Query box, green gaps, saved list |
| **Tooling**     | Ruff Â· Mypy Â· Pytest  | 90 %+ coverage, preâ€‘commit hooks |
| **Deploy**      | Dockerfile            | Uvicorn + Streamlit           | One image â†’ prodâ€‘ready |

---

## ğŸ–¼ Architecture

```mermaid
flowchart LR
    Q(Query) --> S(SearchService)
    S --> P[Downloader]
    P --> E[OpenAI Extractor]
    E --> A[Aggregator]
    A --> J[ResearchReport JSON]
    J -->|CLI| C[Typer]
    J -->|SQLite| D[(DB)]
    J -->|HTTP| F[FastAPI]
    J -->|UI| U[Streamlit]
```

---

## ğŸ“‹ Requirements

- Python 3.10+
- OpenAI API key
- Serper.dev API key (for Google-style search)
- Poetry (for dependency management)

---

## ğŸ› ï¸ Installation

### Using Poetry (recommended)

```bash
git clone https://github.com/your-org/gtmind && cd gtmind
make install                                # poetry deps + tools
```

### Manual Installation

```bash
git clone https://github.com/your-org/gtmind && cd gtmind
python -m pip install -e .
```

### Docker Installation

```bash
docker build -t gtmind .
docker run -p 8000:8000 -p 8501:8501 -e OPENAI_API_KEY=sk-â€¢â€¢â€¢ -e SEARCH_API_KEY=serp_â€¢â€¢â€¢ gtmind
```

---

## âš™ï¸ Configuration

Create a `.env` file in the project root:

```
OPENAI_API_KEY=sk-â€¢â€¢â€¢
SEARCH_API_KEY=serp_â€¢â€¢â€¢
OPENAI_MODEL=gpt-4o           # Optional, defaults to gpt-4o
MAX_SEARCH_RESULTS=10         # Optional, defaults to 10
ENABLE_CACHING=true           # Optional, defaults to true
```

Or set environment variables directly:

```bash
export OPENAI_API_KEY=sk-â€¢â€¢â€¢
export SEARCH_API_KEY=serp_â€¢â€¢â€¢
```

---

## ğŸš€ Quick Start

```bash
make serve &                                # FastAPI at :8000
make ui                                     # Streamlit at :8501
```

### CLI

```bash
# Basic usage
gtmind "AI in retail"

# Save output to file
gtmind "AI in retail" --out retail.json

# Persist to database
gtmind "AI in retail" --save-sqlite reports.db

# Complete example with options
gtmind "AI in retail" \
       --out retail.json \
       --save-sqlite reports.db \
       --model gpt-4o \
       --max-results 15
```

### API

```
# Basic query
GET http://localhost:8000/report?q=AI+in+retail

# With parameters
GET http://localhost:8000/report?q=AI+in+retail&model=gpt-4o&max_results=15
```

<details>
<summary>Response (JSON)</summary>

```json
{
  "query": "AI in retail",
  "trends": [
    {"text": "AI-driven demand forecasting", "sources": [...]}
  ],
  "companies": [
    {"name": "ForecastPro", "context": "", "sources": [...]}
  ],
  "whitespace_opportunities": [
    {"description": "Tier-2 retailers lack AI tools", "sources": [...]}
  ]
}
```
</details>

### UI

```bash
streamlit run src/gtmind/ui/app.py
```

#### UI Features
* 2â€‘column companies  
* Greenâ€‘highlighted whitespace gaps  
* Sidebar of mostâ€‘recent reports (SQLite)
* Source linking for every insight

---

## ğŸ§© How It Works

1. **Search**: Your query is sent to Serper.dev (Google-style search) to find relevant articles
2. **Parse**: Articles are downloaded and cleaned using trafilatura to remove boilerplate
3. **Extract**: GPT-4o analyzes the content to identify trends, companies, and market gaps
4. **Aggregate**: Similar findings are merged and ranked by frequency and relevance
5. **Present**: Results are formatted as a structured research report available via CLI, API, or UI

### Example Use Cases

- **Market Research**: "AI startups in healthcare 2023"
- **Competitive Analysis**: "Shopify alternatives for SaaS businesses"
- **Trend Spotting**: "Emerging technologies in renewable energy"
- **Gap Analysis**: "Underserved segments in online education"

---

## ğŸ“‚ Project Layout

```
src/gtmind/
â”œâ”€ core/                # search, parse, extract, aggregate
â”‚  â”œâ”€ search.py         # Serper.dev integration
â”‚  â”œâ”€ parse.py          # HTML processing & cleaning
â”‚  â”œâ”€ extract.py        # GPT-4o analysis
â”‚  â””â”€ aggregate.py      # Result deduplication & ranking
â”œâ”€ api/                 # FastAPI + Typer CLI
â”‚  â”œâ”€ run.py            # API server entry point
â”‚  â””â”€ cli.py            # Command line interface
â”œâ”€ ui/                  # Streamlit frontâ€‘end
â”‚  â””â”€ app.py            # UI components & logic
â”œâ”€ persistence.py       # SQLite helpers
â”œâ”€ sample_outputs/      # example JSON reports
â””â”€ tests/               # unit + integration
```

---

## â“ Troubleshooting

### Common Issues

- **API Key Invalid**: Ensure your OpenAI and Serper.dev keys are correctly set in env vars
- **Rate Limiting**: If you encounter "429 Too Many Requests", implement exponential backoff or upgrade your API plan
- **Missing Results**: Try increasing the `--max-results` parameter for broader coverage
- **SQLite Errors**: Check file permissions if using `--save-sqlite`

### Logs

Enable debug logging:

```bash
export GTMIND_LOG_LEVEL=DEBUG
gtmind "your query"
```

---

## ğŸ›£ Future Roadmap

* ğŸ” Vector cache of article embeddings for faster reâ€‘runs  
* âœ¨ RAG enrichment for deeper summaries  
* ğŸ’¾ Postgres adapter for multiâ€‘user persistence  
* ğŸŒ OAuthâ€‘guarded web UI & shareable URLs  
* ğŸ¤– Scheduled cron search with email digests  

---

## ğŸ¤ Contributing

```bash
make check   # ruff + mypy
make test    # pytest
```

### Contribution Guidelines

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes and add tests
4. Run the checks: `make check && make test`
5. Submit a pull request

Pull requests welcome â€” please keep CI green! ğŸ‰

---

## ğŸ“œ License

[MIT](LICENSE)  
Â© 2025 Amrit Kochar & contributors
